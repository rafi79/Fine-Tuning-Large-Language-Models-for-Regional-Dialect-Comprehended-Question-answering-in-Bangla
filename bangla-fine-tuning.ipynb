{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"collapsed_sections":["W5tvYglTvHK7"],"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9434183,"sourceType":"datasetVersion","datasetId":5732048},{"sourceId":9494418,"sourceType":"datasetVersion","datasetId":5777207},{"sourceId":5112,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":3900,"modelId":1902}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"widgets":{"application/vnd.jupyter.widget-state+json":{"97e0da55d1b147f9852391fcbe9f2ddf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_645de73d79254325b26984fee09e36e2","IPY_MODEL_eac0c97d63a8439b9db029a7f29e5531","IPY_MODEL_a0f541714336483b83d3c1d17fb03731"],"layout":"IPY_MODEL_e0fbf0e9ca6e41078e29c8eaa9166482"}},"645de73d79254325b26984fee09e36e2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ec45471ebe8d47d4a57ba4c673d48f7c","placeholder":"​","style":"IPY_MODEL_ce572f15cd8c4ac2b1d287587e164055","value":"config.json: 100%"}},"eac0c97d63a8439b9db029a7f29e5531":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3141a37f2a314ac08b32058d9c5b0cdb","max":596,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4987d9805305411d8634033b2882de5f","value":596}},"a0f541714336483b83d3c1d17fb03731":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_50f6fc8305524198a8ddd3b1afe6f06f","placeholder":"​","style":"IPY_MODEL_d3d194e57a1145f6b10e36b44e826587","value":" 596/596 [00:00&lt;00:00, 37.4kB/s]"}},"e0fbf0e9ca6e41078e29c8eaa9166482":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec45471ebe8d47d4a57ba4c673d48f7c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce572f15cd8c4ac2b1d287587e164055":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3141a37f2a314ac08b32058d9c5b0cdb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4987d9805305411d8634033b2882de5f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"50f6fc8305524198a8ddd3b1afe6f06f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3d194e57a1145f6b10e36b44e826587":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"591376ac1f4a450a968de89f0fe2fda4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7a1f77badbfb430aba335b1d2d2c6385","IPY_MODEL_ff7388737f8748429bb827035231706a","IPY_MODEL_a0cce75a3da0464f91d811d632a07397"],"layout":"IPY_MODEL_55ed62a5993345a0889a5a237a58b576"}},"7a1f77badbfb430aba335b1d2d2c6385":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba20b4c04d694220ab0b0a1bd1737b99","placeholder":"​","style":"IPY_MODEL_5ef908a5bd3042e98b6e80d26211d21e","value":"model.safetensors.index.json: 100%"}},"ff7388737f8748429bb827035231706a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b43f9a57f02146aca8b40826e5993ccb","max":25125,"min":0,"orientation":"horizontal","style":"IPY_MODEL_45676e83151f49d8a797bc90d66741b5","value":25125}},"a0cce75a3da0464f91d811d632a07397":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_add10340f7d64915b34850ced9d6531c","placeholder":"​","style":"IPY_MODEL_032077c14f8f4401b19df2e171e329cf","value":" 25.1k/25.1k [00:00&lt;00:00, 1.72MB/s]"}},"55ed62a5993345a0889a5a237a58b576":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba20b4c04d694220ab0b0a1bd1737b99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ef908a5bd3042e98b6e80d26211d21e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b43f9a57f02146aca8b40826e5993ccb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"45676e83151f49d8a797bc90d66741b5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"add10340f7d64915b34850ced9d6531c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"032077c14f8f4401b19df2e171e329cf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"be120c4c83d941eaaac268ac8d3058c0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_73d7e13ff32742f5961dc55d126fca81","IPY_MODEL_4bb2a0fac94645bab239f2c53e6d3f0c","IPY_MODEL_569ca0cef37248b79994324e426283c7"],"layout":"IPY_MODEL_a5a736ef11144f59b496e554821b1a9a"}},"73d7e13ff32742f5961dc55d126fca81":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1d07d25649254b5696bbdb525502709b","placeholder":"​","style":"IPY_MODEL_7f90d4ef258542068a676fe4ef676274","value":"Downloading shards: 100%"}},"4bb2a0fac94645bab239f2c53e6d3f0c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b28aed7f00334fa0b67bdf3da47baf30","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b2590de93e34458784376476d2c50cfb","value":3}},"569ca0cef37248b79994324e426283c7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ff7b0620ed8e4d44823e6e02e726491a","placeholder":"​","style":"IPY_MODEL_6d5de68164b8467bb1e92e5f55d6a34f","value":" 3/3 [01:45&lt;00:00, 35.61s/it]"}},"a5a736ef11144f59b496e554821b1a9a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d07d25649254b5696bbdb525502709b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f90d4ef258542068a676fe4ef676274":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b28aed7f00334fa0b67bdf3da47baf30":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2590de93e34458784376476d2c50cfb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ff7b0620ed8e4d44823e6e02e726491a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d5de68164b8467bb1e92e5f55d6a34f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"11340e48140149ff84af64e54053ce6d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b0b6b9a85d884785917f3f5e5439cfcf","IPY_MODEL_cdb3f7ab11ba464198c04757c79c46a3","IPY_MODEL_ebf5296732844983be1667d34a0aa1a2"],"layout":"IPY_MODEL_105e06b6909d4927bd628bd5b6f099ca"}},"b0b6b9a85d884785917f3f5e5439cfcf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8a232dec06ad47bcbe60e2c468137a30","placeholder":"​","style":"IPY_MODEL_899f338e6bd14c96b57b15813a3357e6","value":"model-00001-of-00003.safetensors: 100%"}},"cdb3f7ab11ba464198c04757c79c46a3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_95c90a205b6647e4b6c73e5c7441091f","max":4943162336,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c82c82afa8cb4dcc8a20c1a4358d5f73","value":4943162336}},"ebf5296732844983be1667d34a0aa1a2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f17e436594f145a5a47f5cd4d5bf37f4","placeholder":"​","style":"IPY_MODEL_8b752e2ad0fd40dfbfdd94cfc50d7c94","value":" 4.94G/4.94G [00:33&lt;00:00, 187MB/s]"}},"105e06b6909d4927bd628bd5b6f099ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a232dec06ad47bcbe60e2c468137a30":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"899f338e6bd14c96b57b15813a3357e6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"95c90a205b6647e4b6c73e5c7441091f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c82c82afa8cb4dcc8a20c1a4358d5f73":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f17e436594f145a5a47f5cd4d5bf37f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b752e2ad0fd40dfbfdd94cfc50d7c94":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f1058cde639f4dbdbbd932288cf6d885":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_eb337f664c7b40c7bc46fe602eefe05a","IPY_MODEL_1513438efc8b4e71a2f1d62f0a148492","IPY_MODEL_bf21031fda8c414fb665f82b8f431683"],"layout":"IPY_MODEL_9e19f080e6cb498ab3ec12e14004c2ea"}},"eb337f664c7b40c7bc46fe602eefe05a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_074be93eba684635a5ec262540dd39c7","placeholder":"​","style":"IPY_MODEL_059d5b8320024e9cb2d8c4621d988004","value":"model-00002-of-00003.safetensors: 100%"}},"1513438efc8b4e71a2f1d62f0a148492":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba747fdfea8f45b29069521cbc165175","max":4999819336,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a5898ca12a5544dba1803e8f9d9c76e4","value":4999819336}},"bf21031fda8c414fb665f82b8f431683":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2a77fce2bc474e9aa55335a6a261ca3a","placeholder":"​","style":"IPY_MODEL_8b8c91b349b4492cb801540a3596adbc","value":" 5.00G/5.00G [00:35&lt;00:00, 247MB/s]"}},"9e19f080e6cb498ab3ec12e14004c2ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"074be93eba684635a5ec262540dd39c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"059d5b8320024e9cb2d8c4621d988004":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ba747fdfea8f45b29069521cbc165175":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a5898ca12a5544dba1803e8f9d9c76e4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2a77fce2bc474e9aa55335a6a261ca3a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b8c91b349b4492cb801540a3596adbc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fbe5dabe6d004d6cbd3cb88fe504f2b6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1731e84193fa4af6935750383cf1e29c","IPY_MODEL_443cb7b46ffd483885bd92de7f84cf58","IPY_MODEL_822e9b896b03488cbf4f8f51a832210a"],"layout":"IPY_MODEL_1fe64b3643994df8a8a8fb18f87dfb47"}},"1731e84193fa4af6935750383cf1e29c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6bb8229c99864552817d0745cee3e38f","placeholder":"​","style":"IPY_MODEL_8b465c725cde4e03a8fb7fd6461ad2b9","value":"model-00003-of-00003.safetensors: 100%"}},"443cb7b46ffd483885bd92de7f84cf58":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_795bee72ec91457a87d5039d80be194b","max":4540516344,"min":0,"orientation":"horizontal","style":"IPY_MODEL_49e77f796d374b6d81fd04382dec436b","value":4540516344}},"822e9b896b03488cbf4f8f51a832210a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d95fe70fb25442eab859ecd5c7da4e5b","placeholder":"​","style":"IPY_MODEL_530a42ff7afc42479227368994cfe229","value":" 4.54G/4.54G [00:36&lt;00:00, 148MB/s]"}},"1fe64b3643994df8a8a8fb18f87dfb47":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6bb8229c99864552817d0745cee3e38f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b465c725cde4e03a8fb7fd6461ad2b9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"795bee72ec91457a87d5039d80be194b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"49e77f796d374b6d81fd04382dec436b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d95fe70fb25442eab859ecd5c7da4e5b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"530a42ff7afc42479227368994cfe229":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"93a9a683e4ae44fba1e104df47db65fb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0c64169bf86249c78c5895a6a0be8b48","IPY_MODEL_3b07be768523442e8f5435a12b0723ab","IPY_MODEL_0f71234ccda44f78bf8e70412d6fe61a"],"layout":"IPY_MODEL_3c4e9489a1ce4789980a4c6bf0ebca2c"}},"0c64169bf86249c78c5895a6a0be8b48":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_43952c001391401dac5d7d8747477f4f","placeholder":"​","style":"IPY_MODEL_84b714d608b849349504b93be51b0c2c","value":"Loading checkpoint shards: 100%"}},"3b07be768523442e8f5435a12b0723ab":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_34d441d80308404385f4287ccc4e3d67","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2ab23dd2ea364cec91f47060412a58d8","value":3}},"0f71234ccda44f78bf8e70412d6fe61a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2fa6ea0ebfc44b93a09cc8c979a7c4d6","placeholder":"​","style":"IPY_MODEL_f9dc359a429a4f998302ffc133c649d2","value":" 3/3 [01:06&lt;00:00, 21.82s/it]"}},"3c4e9489a1ce4789980a4c6bf0ebca2c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"43952c001391401dac5d7d8747477f4f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84b714d608b849349504b93be51b0c2c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"34d441d80308404385f4287ccc4e3d67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ab23dd2ea364cec91f47060412a58d8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2fa6ea0ebfc44b93a09cc8c979a7c4d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9dc359a429a4f998302ffc133c649d2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"92c8ae082a37414a9929702a73c301f3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_be1afe13214d4104aedb0b9118c9d89c","IPY_MODEL_3cba21e2ca7549ecac5602d9c9bad983","IPY_MODEL_84f255dbcb514c2aaf7048fee6fd8cd8"],"layout":"IPY_MODEL_74faa4ee95be40bbb5eef4c6e770cd5c"}},"be1afe13214d4104aedb0b9118c9d89c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c0d6c2ac35ff4b1b9a5db4ed30eea508","placeholder":"​","style":"IPY_MODEL_58e3730bd6174bc0bf6834465b5c08c7","value":"generation_config.json: 100%"}},"3cba21e2ca7549ecac5602d9c9bad983":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_32107c37162d42b1ab83756de5981575","max":111,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f79077d0a0d947d89e3ec73df302ecc5","value":111}},"84f255dbcb514c2aaf7048fee6fd8cd8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_60cf07ea2d8e4d81adcb3717d162120d","placeholder":"​","style":"IPY_MODEL_008c0617371744a3b567be1e5d3dd482","value":" 111/111 [00:00&lt;00:00, 7.62kB/s]"}},"74faa4ee95be40bbb5eef4c6e770cd5c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0d6c2ac35ff4b1b9a5db4ed30eea508":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58e3730bd6174bc0bf6834465b5c08c7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"32107c37162d42b1ab83756de5981575":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f79077d0a0d947d89e3ec73df302ecc5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"60cf07ea2d8e4d81adcb3717d162120d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"008c0617371744a3b567be1e5d3dd482":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"69134ceedfc3424081b7469f2e88eb34":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cc0455594470489ca7c96b82df549a36","IPY_MODEL_0b0efebe27424f50bdc014dbc97b6dec","IPY_MODEL_85a88bd8484e40698d373b1823a52996"],"layout":"IPY_MODEL_e696c94f2e8d478590fdc114f7a0ea09"}},"cc0455594470489ca7c96b82df549a36":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b302a9d8b1144ae0b3cd589623c2ee2c","placeholder":"​","style":"IPY_MODEL_16a6a6b75168440499d26b78a53ff0b5","value":"tokenizer_config.json: 100%"}},"0b0efebe27424f50bdc014dbc97b6dec":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_79fa164b934f4b84b9094880531ce184","max":1460,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bf6aa81a24ae44c1ab2658f59a0dcda1","value":1460}},"85a88bd8484e40698d373b1823a52996":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5e8a8a2af0764831a8cab311bd3ec25a","placeholder":"​","style":"IPY_MODEL_4a4b7ef2a3fd4f1ba61689c126beefbb","value":" 1.46k/1.46k [00:00&lt;00:00, 74.0kB/s]"}},"e696c94f2e8d478590fdc114f7a0ea09":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b302a9d8b1144ae0b3cd589623c2ee2c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16a6a6b75168440499d26b78a53ff0b5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"79fa164b934f4b84b9094880531ce184":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf6aa81a24ae44c1ab2658f59a0dcda1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5e8a8a2af0764831a8cab311bd3ec25a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a4b7ef2a3fd4f1ba61689c126beefbb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c9dc32b57c4444398aa38ddeb9fed7ec":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_760dffb29ac24c63bfef8f0746cde43f","IPY_MODEL_8cf9eafd152f4d09a31a79b7b2624340","IPY_MODEL_a88120ce148747498cf61d1ab96058d3"],"layout":"IPY_MODEL_a326197d48584106a5d0d18d504005c1"}},"760dffb29ac24c63bfef8f0746cde43f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_24d21e7b8b6a40c5a78d5a51d8255fee","placeholder":"​","style":"IPY_MODEL_b76bcdb5b9ce45739dbe2376789292e7","value":"tokenizer.model: 100%"}},"8cf9eafd152f4d09a31a79b7b2624340":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c09d93180e9e45b6a0f063a7962c5660","max":493443,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c6fcfac0e507460a8bb7e1173a3ffbf1","value":493443}},"a88120ce148747498cf61d1ab96058d3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8a80772c9ac245699fe22c76f671ed7d","placeholder":"​","style":"IPY_MODEL_f90c078a20a24d1ea4a8b450945c4522","value":" 493k/493k [00:00&lt;00:00, 28.8MB/s]"}},"a326197d48584106a5d0d18d504005c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"24d21e7b8b6a40c5a78d5a51d8255fee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b76bcdb5b9ce45739dbe2376789292e7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c09d93180e9e45b6a0f063a7962c5660":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6fcfac0e507460a8bb7e1173a3ffbf1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8a80772c9ac245699fe22c76f671ed7d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f90c078a20a24d1ea4a8b450945c4522":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c5cf28cb1f6641c6aec4cd60d9809e7e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_145a31068e3649a79c142842951df354","IPY_MODEL_3ac10de3bb6a421c96a0f88dd20db6d3","IPY_MODEL_f2d38e698a5c4ce1a4650cea52f4b0b8"],"layout":"IPY_MODEL_9f1f3f714c274679ace5f96c519c5aa0"}},"145a31068e3649a79c142842951df354":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1febef6f23854af8a438848a06d5cdcc","placeholder":"​","style":"IPY_MODEL_f475b1fa9c984223b0e5f72e6d0f3503","value":"tokenizer.json: 100%"}},"3ac10de3bb6a421c96a0f88dd20db6d3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d3567c67c74d4d94abc4e20fd6b401ee","max":1795303,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e6ed23b170d34a75ae5e8234468cf276","value":1795303}},"f2d38e698a5c4ce1a4650cea52f4b0b8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e52d073995114cd3a89ad6b6f1d53dfc","placeholder":"​","style":"IPY_MODEL_6f909c8b2a1d4cdfab8267233301ed55","value":" 1.80M/1.80M [00:00&lt;00:00, 5.51MB/s]"}},"9f1f3f714c274679ace5f96c519c5aa0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1febef6f23854af8a438848a06d5cdcc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f475b1fa9c984223b0e5f72e6d0f3503":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d3567c67c74d4d94abc4e20fd6b401ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6ed23b170d34a75ae5e8234468cf276":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e52d073995114cd3a89ad6b6f1d53dfc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f909c8b2a1d4cdfab8267233301ed55":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c6b5ef0fc40c4331b324dbc58743d3fc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1d549c1088c64e87a1bfdc2160ec9b98","IPY_MODEL_87e374450e294e83ad1492fd17ad51d9","IPY_MODEL_4a6ef7557a1f4f499d7f1f52bdce80e5"],"layout":"IPY_MODEL_18e5849b83e4409ebbd1dfd9ee64fb85"}},"1d549c1088c64e87a1bfdc2160ec9b98":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8fb9a7f1214e450e9dfc2fd1a3591a5f","placeholder":"​","style":"IPY_MODEL_16d2462195d34aa595bc526b544afb91","value":"special_tokens_map.json: 100%"}},"87e374450e294e83ad1492fd17ad51d9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_77d7e015a3204656bf597b2fb2360c00","max":72,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6f7ccc8db40f470c9ad01637ce65ac70","value":72}},"4a6ef7557a1f4f499d7f1f52bdce80e5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_56c28bf740e14744a8d53484a3cb8ae1","placeholder":"​","style":"IPY_MODEL_5e8478fe39304e3babafcfa822169f60","value":" 72.0/72.0 [00:00&lt;00:00, 5.49kB/s]"}},"18e5849b83e4409ebbd1dfd9ee64fb85":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8fb9a7f1214e450e9dfc2fd1a3591a5f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16d2462195d34aa595bc526b544afb91":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"77d7e015a3204656bf597b2fb2360c00":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f7ccc8db40f470c9ad01637ce65ac70":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"56c28bf740e14744a8d53484a3cb8ae1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e8478fe39304e3babafcfa822169f60":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"feb4882e16304ce79db973d36a845222":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c1d9cc4e27e84e38a80fd36a5f0b4ca7","IPY_MODEL_474661a662fa44cda76e4b6585baf564","IPY_MODEL_65b050dfa3c74b44a552794ab1a24e96"],"layout":"IPY_MODEL_0f0943e3767a4e75afa0a06b027cf129"}},"c1d9cc4e27e84e38a80fd36a5f0b4ca7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8cfc04e59f748fb96d7769f06416777","placeholder":"​","style":"IPY_MODEL_5adbe6c3007f447382f3d8e24dc49904","value":"Downloading readme: 100%"}},"474661a662fa44cda76e4b6585baf564":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0b76e44cf8ee42499e7e7cb791fcb719","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5db6e85fa995448a917b998c6a8444ad","value":28}},"65b050dfa3c74b44a552794ab1a24e96":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7bdff59a76834f69bb4b2deb99ae2246","placeholder":"​","style":"IPY_MODEL_9e5862c3935c4a0eb8bc081aeab87fc1","value":" 28.0/28.0 [00:00&lt;00:00, 1.67kB/s]"}},"0f0943e3767a4e75afa0a06b027cf129":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8cfc04e59f748fb96d7769f06416777":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5adbe6c3007f447382f3d8e24dc49904":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0b76e44cf8ee42499e7e7cb791fcb719":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5db6e85fa995448a917b998c6a8444ad":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7bdff59a76834f69bb4b2deb99ae2246":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e5862c3935c4a0eb8bc081aeab87fc1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"82e76634360048a081b549a8d60108d4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_792e7bd2bd554c40867f7bb19c36db79","IPY_MODEL_645884190ae94a9a94197e2059992cd6","IPY_MODEL_f7e30feeeb474823a0dc52e9578ad910"],"layout":"IPY_MODEL_3f4710df470944338f251a593a06ad42"}},"792e7bd2bd554c40867f7bb19c36db79":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_019e00b7855143fa9a324c3c10d0251f","placeholder":"​","style":"IPY_MODEL_25537a57192f49ee8b090168cba3e329","value":"Downloading data: 100%"}},"645884190ae94a9a94197e2059992cd6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc1c72012cdc45ab82e704f72193f843","max":22313065,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ca2d1d3c9f9c4779a5775348e55e511e","value":22313065}},"f7e30feeeb474823a0dc52e9578ad910":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f22b7439fce04cf6815d912c9c67e344","placeholder":"​","style":"IPY_MODEL_72ba45ce5f424cc68ee1f9fe292b4b44","value":" 22.3M/22.3M [00:00&lt;00:00, 38.6MB/s]"}},"3f4710df470944338f251a593a06ad42":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"019e00b7855143fa9a324c3c10d0251f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"25537a57192f49ee8b090168cba3e329":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cc1c72012cdc45ab82e704f72193f843":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca2d1d3c9f9c4779a5775348e55e511e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f22b7439fce04cf6815d912c9c67e344":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"72ba45ce5f424cc68ee1f9fe292b4b44":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3846799669a5451fba3bbb0e6738c99c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_33a9b1d86bd34aedb9061b72442a105f","IPY_MODEL_a24d9a99c5c04c21ab62d6c66b992982","IPY_MODEL_e034cde57048456296798b1f94da5d40"],"layout":"IPY_MODEL_7ef38577fe7947448d890da7d7d6b1ea"}},"33a9b1d86bd34aedb9061b72442a105f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f814b454b74a4c10b1dc312b339a14d0","placeholder":"​","style":"IPY_MODEL_e874a50d94394829b58e234488afc2f8","value":"Generating train split: "}},"a24d9a99c5c04c21ab62d6c66b992982":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0498506bcdac47ccbadcd03bcdf1b47a","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_36160858837c4ffc8beb1082f4c6509f","value":1}},"e034cde57048456296798b1f94da5d40":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c4db3b1f980b4781b94d7bee4a352322","placeholder":"​","style":"IPY_MODEL_5e277b6e736441f1a23eacc0ac54b864","value":" 15519/0 [00:00&lt;00:00, 39766.92 examples/s]"}},"7ef38577fe7947448d890da7d7d6b1ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f814b454b74a4c10b1dc312b339a14d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e874a50d94394829b58e234488afc2f8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0498506bcdac47ccbadcd03bcdf1b47a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"36160858837c4ffc8beb1082f4c6509f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c4db3b1f980b4781b94d7bee4a352322":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e277b6e736441f1a23eacc0ac54b864":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install -U datasets","metadata":{"id":"9-T--O9RUrdp"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q accelerate peft bitsandbytes transformers trl","metadata":{"id":"lvK-uypZJm4k","colab":{"base_uri":"https://localhost:8080/"},"outputId":"54d4e15a-d6fc-4c91-c2bf-325cf3c05af8","execution":{"iopub.status.busy":"2024-09-28T10:43:15.387783Z","iopub.execute_input":"2024-09-28T10:43:15.388203Z","iopub.status.idle":"2024-09-28T10:43:30.745225Z","shell.execute_reply.started":"2024-09-28T10:43:15.388156Z","shell.execute_reply":"2024-09-28T10:43:30.743930Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import math\nimport os\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom datasets import load_dataset, Dataset\nfrom transformers import (AutoModelForCausalLM,\n                   AutoTokenizer,\n                   BitsAndBytesConfig,\n                   HfArgumentParser,\n                   TrainingArguments,\n                   TextDataset,\n                   DataCollatorForLanguageModeling,\n                   pipeline,\n                   logging)\nfrom accelerate import Accelerator\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer","metadata":{"id":"01AQgGOXJ_Un","execution":{"iopub.status.busy":"2024-09-28T10:43:30.746939Z","iopub.execute_input":"2024-09-28T10:43:30.747283Z","iopub.status.idle":"2024-09-28T10:43:48.417810Z","shell.execute_reply.started":"2024-09-28T10:43:30.747247Z","shell.execute_reply":"2024-09-28T10:43:48.416922Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# promt format for Mistral\n# <s> [INST] <<SYS>> System_Promt <</SYS>> User_prompt [/INST] LLM_Response </s>","metadata":{"id":"p8edyp33K1AO"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Preprocessing\n\n> Add blockquote\n\n","metadata":{"id":"W5tvYglTvHK7","jp-MarkdownHeadingCollapsed":true,"tags":[]}},{"cell_type":"code","source":"from google.colab import drive\ndrive.mount('/content/drive')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aiIaKqvRAUcb","outputId":"dbeb1f3b-78fc-4e40-b633-8abfae44f7ea"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"remove_words = [\"উত্তর-\", \"উঃ।\"]\n\nwith open('data_chatbot.txt', 'w', encoding=\"utf8\") as f:\n    for index, item in data_chatbot.iterrows():\n        answer = item[\"উত্তর\"].replace('</s>', \"\").strip()\n        question =\"<s>[INST]\"+\" \"+item[\"প্রশ্ন\"].strip()+\"[/INST]\"\n        #             temp_data = {\"Question\": question, \"Answer\": answer}\n        #             csv_data = csv_data.append(temp_data, ignore_index = True)\n        data_qa = question+\" \"+answer\n        f.write(data_qa.replace('\\n', \"\").replace('\\r', \"\")+\" </s>\\n\")","metadata":{"id":"pQbyQghQ_py2"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"remove_words = [\"উত্তর-\", \"উঃ।\"]\n\nwith open('data_processed.txt', 'w', encoding=\"utf8\") as f:\n  for index, item in data.iterrows():\n    answer = data2[\"exp_bangla\"][index]\n    if type(answer).__name__ == \"str\":\n      question =\"<s>[INST]\"+\" \"+item[\"question_bangla\"].strip()+\"[/INST]\"\n#       question = item[\"question_bangla\"]\n\n      for rm_w in remove_words:\n          answer = answer.replace(rm_w, \"\").replace('</s>', '').strip()\n#         temp_data = {\"Question\": question, \"Answer\": answer}\n#         csv_data = csv_data.append(temp_data, ignore_index = True)\n          data_qa = question+\" \"+answer\n          f.write(data_qa.replace('\\n', \"\").replace('\\r', \"\")+\"</s>\\n\")","metadata":{"id":"3FBSvp6u6-DA"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LLM Model","metadata":{"id":"IQBu1xZmC6_h"}},{"cell_type":"code","source":"# The model that you want to train from the Hugging Face hub\nmodel_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n\n# Fine-tuned model name\nnew_model = \"Mistral-7b-QA\"\n\n# QLoRA parameters\n\n# LoRA attention dimension\nlora_r = 64\n\n# Alpha parameter for LoRA scaling\nlora_alpha = 16\n\n# Dropout probability for LoRA layers\nlora_dropout = 0.1\n\n# bitsandbytes parameters\n\n# Activate 4-bit precision base model loading\nuse_4bit = True\n\n# Compute dtype for 4-bit base models\nbnb_4bit_compute_dtype = \"float16\"\n\n# Quantization type (fp4 or nf4)\nbnb_4bit_quant_type = \"nf4\"\n\n# Activate nested quantization for 4-bit base models (double quantization)\nuse_nested_quant = False\n\n# TrainingArguments parameters\n\n# Output directory where the model predictions and checkpoints will be stored\noutput_dir = \"./results\"\n\n# Enable gradient checkpointing\ngradient_checkpointing = True\n\n# Maximum gradient normal (gradient clipping)\nmax_grad_norm = 0.3\n\n# Initial learning rate (AdamW optimizer)\nlearning_rate = 2e-4\n\n# Weight decay to apply to all layers except bias/LayerNorm weights\nweight_decay = 0.001\n\n# Optimizer to use\noptim = \"paged_adamw_32bit\"\n\n# Learning rate schedule\nlr_scheduler_type = \"cosine\"\n\n# Number of training steps (overrides num_train_epochs)\nmax_steps = -1\n\n# Ratio of steps for a linear warmup (from 0 to learning rate)\nwarmup_ratio = 0.03\n\n# Group sequences into batches with same length\n# Saves memory and speeds up training considerably\ngroup_by_length = True\n\n# Save checkpoint every X updates steps\nsave_steps = 0\n\n# SFT parameters\n\n# Pack multiple short examples in the same input sequence to increase efficiency\npacking = False\n\n# Load the entire model on the GPU 0\ndevice_map = {\"\": 0}","metadata":{"id":"QWTpQl8CC6ed","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load tokenizer and model with QLoRA configuration\ncompute_dtype = getattr(torch, bnb_4bit_compute_dtype)\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=use_4bit,\n    bnb_4bit_quant_type=bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=use_nested_quant,\n)\n\n# bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\n\n# Check GPU compatibility with bfloat16\nif compute_dtype == torch.float16 and use_4bit:\n    major, _ = torch.cuda.get_device_capability()\n    if major >= 8:\n        print(\"=\" * 80)\n        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n        print(\"=\" * 80)\n\n# Load base model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=device_map\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\n# Load LLaMA tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n\n# Load LoRA configuration\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules = [\n    \"lm_head\",\n    \"q_proj\",\n    \"k_proj\",\n    \"v_proj\",\n    \"up_proj\",\n    \"down_proj\",\n    \"o_proj\",\n    \"gate_proj\"\n  ]\n)","metadata":{"id":"rpXqa3iXDKIm","outputId":"747f27e8-3154-41d0-8ee5-159dc0026a44","colab":{"referenced_widgets":["97e0da55d1b147f9852391fcbe9f2ddf","645de73d79254325b26984fee09e36e2","eac0c97d63a8439b9db029a7f29e5531","a0f541714336483b83d3c1d17fb03731","e0fbf0e9ca6e41078e29c8eaa9166482","ec45471ebe8d47d4a57ba4c673d48f7c","ce572f15cd8c4ac2b1d287587e164055","3141a37f2a314ac08b32058d9c5b0cdb","4987d9805305411d8634033b2882de5f","50f6fc8305524198a8ddd3b1afe6f06f","d3d194e57a1145f6b10e36b44e826587","591376ac1f4a450a968de89f0fe2fda4","7a1f77badbfb430aba335b1d2d2c6385","ff7388737f8748429bb827035231706a","a0cce75a3da0464f91d811d632a07397","55ed62a5993345a0889a5a237a58b576","ba20b4c04d694220ab0b0a1bd1737b99","5ef908a5bd3042e98b6e80d26211d21e","b43f9a57f02146aca8b40826e5993ccb","45676e83151f49d8a797bc90d66741b5","add10340f7d64915b34850ced9d6531c","032077c14f8f4401b19df2e171e329cf","be120c4c83d941eaaac268ac8d3058c0","73d7e13ff32742f5961dc55d126fca81","4bb2a0fac94645bab239f2c53e6d3f0c","569ca0cef37248b79994324e426283c7","a5a736ef11144f59b496e554821b1a9a","1d07d25649254b5696bbdb525502709b","7f90d4ef258542068a676fe4ef676274","b28aed7f00334fa0b67bdf3da47baf30","b2590de93e34458784376476d2c50cfb","ff7b0620ed8e4d44823e6e02e726491a","6d5de68164b8467bb1e92e5f55d6a34f","11340e48140149ff84af64e54053ce6d","b0b6b9a85d884785917f3f5e5439cfcf","cdb3f7ab11ba464198c04757c79c46a3","ebf5296732844983be1667d34a0aa1a2","105e06b6909d4927bd628bd5b6f099ca","8a232dec06ad47bcbe60e2c468137a30","899f338e6bd14c96b57b15813a3357e6","95c90a205b6647e4b6c73e5c7441091f","c82c82afa8cb4dcc8a20c1a4358d5f73","f17e436594f145a5a47f5cd4d5bf37f4","8b752e2ad0fd40dfbfdd94cfc50d7c94","f1058cde639f4dbdbbd932288cf6d885","eb337f664c7b40c7bc46fe602eefe05a","1513438efc8b4e71a2f1d62f0a148492","bf21031fda8c414fb665f82b8f431683","9e19f080e6cb498ab3ec12e14004c2ea","074be93eba684635a5ec262540dd39c7","059d5b8320024e9cb2d8c4621d988004","ba747fdfea8f45b29069521cbc165175","a5898ca12a5544dba1803e8f9d9c76e4","2a77fce2bc474e9aa55335a6a261ca3a","8b8c91b349b4492cb801540a3596adbc","fbe5dabe6d004d6cbd3cb88fe504f2b6","1731e84193fa4af6935750383cf1e29c","443cb7b46ffd483885bd92de7f84cf58","822e9b896b03488cbf4f8f51a832210a","1fe64b3643994df8a8a8fb18f87dfb47","6bb8229c99864552817d0745cee3e38f","8b465c725cde4e03a8fb7fd6461ad2b9","795bee72ec91457a87d5039d80be194b","49e77f796d374b6d81fd04382dec436b","d95fe70fb25442eab859ecd5c7da4e5b","530a42ff7afc42479227368994cfe229","93a9a683e4ae44fba1e104df47db65fb","0c64169bf86249c78c5895a6a0be8b48","3b07be768523442e8f5435a12b0723ab","0f71234ccda44f78bf8e70412d6fe61a","3c4e9489a1ce4789980a4c6bf0ebca2c","43952c001391401dac5d7d8747477f4f","84b714d608b849349504b93be51b0c2c","34d441d80308404385f4287ccc4e3d67","2ab23dd2ea364cec91f47060412a58d8","2fa6ea0ebfc44b93a09cc8c979a7c4d6","f9dc359a429a4f998302ffc133c649d2","92c8ae082a37414a9929702a73c301f3","be1afe13214d4104aedb0b9118c9d89c","3cba21e2ca7549ecac5602d9c9bad983","84f255dbcb514c2aaf7048fee6fd8cd8","74faa4ee95be40bbb5eef4c6e770cd5c","c0d6c2ac35ff4b1b9a5db4ed30eea508","58e3730bd6174bc0bf6834465b5c08c7","32107c37162d42b1ab83756de5981575","f79077d0a0d947d89e3ec73df302ecc5","60cf07ea2d8e4d81adcb3717d162120d","008c0617371744a3b567be1e5d3dd482","69134ceedfc3424081b7469f2e88eb34","cc0455594470489ca7c96b82df549a36","0b0efebe27424f50bdc014dbc97b6dec","85a88bd8484e40698d373b1823a52996","e696c94f2e8d478590fdc114f7a0ea09","b302a9d8b1144ae0b3cd589623c2ee2c","16a6a6b75168440499d26b78a53ff0b5","79fa164b934f4b84b9094880531ce184","bf6aa81a24ae44c1ab2658f59a0dcda1","5e8a8a2af0764831a8cab311bd3ec25a","4a4b7ef2a3fd4f1ba61689c126beefbb","c9dc32b57c4444398aa38ddeb9fed7ec","760dffb29ac24c63bfef8f0746cde43f","8cf9eafd152f4d09a31a79b7b2624340","a88120ce148747498cf61d1ab96058d3","a326197d48584106a5d0d18d504005c1","24d21e7b8b6a40c5a78d5a51d8255fee","b76bcdb5b9ce45739dbe2376789292e7","c09d93180e9e45b6a0f063a7962c5660","c6fcfac0e507460a8bb7e1173a3ffbf1","8a80772c9ac245699fe22c76f671ed7d","f90c078a20a24d1ea4a8b450945c4522","c5cf28cb1f6641c6aec4cd60d9809e7e","145a31068e3649a79c142842951df354","3ac10de3bb6a421c96a0f88dd20db6d3","f2d38e698a5c4ce1a4650cea52f4b0b8","9f1f3f714c274679ace5f96c519c5aa0","1febef6f23854af8a438848a06d5cdcc","f475b1fa9c984223b0e5f72e6d0f3503","d3567c67c74d4d94abc4e20fd6b401ee","e6ed23b170d34a75ae5e8234468cf276","e52d073995114cd3a89ad6b6f1d53dfc","6f909c8b2a1d4cdfab8267233301ed55","c6b5ef0fc40c4331b324dbc58743d3fc","1d549c1088c64e87a1bfdc2160ec9b98","87e374450e294e83ad1492fd17ad51d9","4a6ef7557a1f4f499d7f1f52bdce80e5","18e5849b83e4409ebbd1dfd9ee64fb85","8fb9a7f1214e450e9dfc2fd1a3591a5f","16d2462195d34aa595bc526b544afb91","77d7e015a3204656bf597b2fb2360c00","6f7ccc8db40f470c9ad01637ce65ac70","56c28bf740e14744a8d53484a3cb8ae1","5e8478fe39304e3babafcfa822169f60"],"base_uri":"https://localhost:8080/","height":525},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"max_seq_length = model.config.max_position_embeddings\nprint(f\"Model can handle maximum {max_seq_length} tokens\")\n\nlength = [len(tokenizer.encode(data[\"text\"])) for data in dataset]\nprint(\"Max sequence length of the dataset is {}\".format(max(length)))\n# chatbot 785\n# old 930","metadata":{"id":"NyDWf2bTUrd7","outputId":"e5b2470c-b6e3-4c11-fed4-96d3137d1479","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# For Training Only","metadata":{"id":"Bisuz7vOUrd9"}},{"cell_type":"markdown","source":"# Inference","metadata":{"id":"M3oPrTv-MRTC"}},{"cell_type":"code","source":"peft_model_id = \"/content/drive/MyDrive/dataset_chatbot/mistral_qa/Mistral-7b-QA\"\nmodel.load_adapter(peft_model_id)\n\ndef filter_response(response):\n    list_res = list(response.rsplit(\"।\", -1))\n    unique_parts = set(list_res)\n\n    index_tuple = []\n\n    for sentence in unique_parts:\n        if sentence in list_res:\n            index = list_res.index(sentence)\n            index_tuple.append((sentence, index))\n\n    sorted_list = sorted(index_tuple, key=lambda x: x[1])\n\n    if len(sorted_list) > 3:\n        sorted_list = sorted_list[:3]\n\n    final_res = \"\"\n    for sentence in sorted_list:\n        final_res += (sentence[0]+\"।\")\n\n    return final_res\n\ndef generate_text(prompt, model, tokenizer):\n    prompt = f\"<s>[INST] {prompt}[/INST]\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n    with torch.no_grad():\n        generated_code = tokenizer.decode(model.generate(**inputs, max_new_tokens=max_seq_length, pad_token_id=2)[0], skip_special_tokens=True)\n#     print(generated_code)\n    return filter_response(generated_code.rsplit('[/INST]', 1)[1].strip())","metadata":{"id":"DEK-vg1dUrd-"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"question = \"আমার বাচ্চার বয়স মাত্র ৪ মাস। সে এখনও শাকসবজি খাওয়া শুরু করেনি। কি করতে পারি?\"\nresponse = generate_text(question, model, tokenizer)\nprint(response)","metadata":{"id":"tC6zG0ToUrd-","outputId":"456fef06-80cd-4af8-e89e-fb24d127478f","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"question = \"আমার ৩ বছর এর বাচ্চা থার্মোমিটার মুখ দিয়ে ভেঙ্গে ফেলেছে। মুখে একটু কেটে গেছে। এখন কি করব?\"\nresponse = generate_text(question, model, tokenizer)\nprint(response)","metadata":{"id":"Ehb-i5b1Ir-E","outputId":"a52cdaa5-ad48-4adf-be25-f73b9a5843c4","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"question = \"বাচ্চার সর্দি কাশি হলে কি কমলা বা মাল্টা খাওয়ানো যাবে?\"\nresponse = generate_text(question, model, tokenizer)\nprint(response)","metadata":{"id":"tlGhMfflUrd_","outputId":"2d642e58-2b4a-466c-cfe6-601d72a0eb39","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"question = \"আমার শিশু একটি আপেলের টুকরো গিলে ফেলেছে এবং এটি ভুল পথে চলে গেছে। এখন ঠিক আছে বলে মনে হচ্ছে, কিন্তু আমার কি চিন্তা করা উচিত?\"\nresponse = generate_text(question, model, tokenizer)\nprint(response)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zs5rOTKA_xQx","outputId":"4473d26c-8193-471f-a5ef-96988a189e1f"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"question = \"আমার বাচ্চাকে দিনে কত বার খাওানো উচিৎ? বাচ্ছার বয়স ৩ বছর।\"\nresponse = generate_text(question, model, tokenizer)\nprint(response)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QzxdzwSuAMPz","outputId":"5424517b-8d57-4a6c-abc8-f1f5acb6a2dc"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"question = \"বাচ্চাদের মধ্যে অ্যাসপিরেশন কিভাবে চিকিৎসা করা হয়?\"\nresponse = generate_text(question, model, tokenizer)\nprint(response)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n7dRasS2BwiZ","outputId":"04d07ca1-dc98-4f0b-9374-d872ff447fe8"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"question = \"অ্যাসপিরেশন কিভাবে চিকিৎসা করা হয়?\"\nresponse = generate_text(question, model, tokenizer)\nprint(response)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pS78Q4D7DIWp","outputId":"6c3829d1-cff1-4c6d-c0ba-70ba80680b18"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"question = \"অ্যাসপিরেশন কিভাবে চিকিৎসা করা হয়?\"\nresponse = generate_text(question, model, tokenizer)\nprint(response)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gvgtbX3aFDQR","outputId":"2705aefa-d467-47af-d8a5-7161c3388be0"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import necessary libraries\nimport torch\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    TrainingArguments,\n    TrainerCallback,\n    TrainerState,\n    TrainerControl,\n)\nfrom peft import LoraConfig, get_peft_model, TaskType\nfrom datasets import load_dataset\nfrom trl import SFTTrainer\nimport math","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install -U bitsandbytes\n","metadata":{"execution":{"iopub.status.busy":"2024-09-28T10:42:57.002241Z","iopub.execute_input":"2024-09-28T10:42:57.003013Z","iopub.status.idle":"2024-09-28T10:43:15.385282Z","shell.execute_reply.started":"2024-09-28T10:42:57.002973Z","shell.execute_reply":"2024-09-28T10:43:15.384148Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.44.0-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.44.0-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.44.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Import necessary libraries\nimport torch\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    TrainingArguments,\n    BitsAndBytesConfig,\n)\nfrom peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\nfrom datasets import load_dataset\nfrom trl import SFTTrainer\nfrom accelerate import infer_auto_device_map, init_empty_weights\n\n# Set environment variable to reduce fragmentation\nimport os\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n\n# Check if CUDA is available\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# The model that you want to train from the Hugging Face hub\nmodel_name = \"/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1\"\n\n# Fine-tuned model name\nnew_model = \"Mistral-7b-QA\"\n\n# QLoRA parameters\nlora_r = 32  # Reduced to save memory\nlora_alpha = 16\nlora_dropout = 0.05  # Reduced dropout\ntarget_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]  # Updated target modules\n\n# bitsandbytes parameters\nuse_4bit = True\nbnb_4bit_compute_dtype = \"float16\"\nbnb_4bit_quant_type = \"nf4\"\nuse_nested_quant = False\n\n# TrainingArguments parameters\noutput_dir = \"./results\"\ngradient_checkpointing = True\nmax_grad_norm = 0.3\nlearning_rate = 2e-4\nweight_decay = 0.001\noptim = \"paged_adamw_32bit\"\nlr_scheduler_type = \"cosine\"\nmax_steps = -1\nwarmup_ratio = 0.03\ngroup_by_length = True\nsave_steps = 0\n\n# SFT parameters\npacking = False\nnum_train_epochs = 1\nfp16 = True\nbf16 = False\nper_device_train_batch_size = 1\nper_device_eval_batch_size = 1\ngradient_accumulation_steps = 4\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\n    model_name,\n    trust_remote_code=True,\n    padding_side=\"left\",\n    use_fast=False,\n    add_bos_token=False,\n    add_eos_token=False,\n)\ntokenizer.pad_token = tokenizer.eos_token  # Ensure pad token is set\n\n# Load the dataset\ndataset_name = \"/kaggle/input/noahkalifinal/Noakhali Train Translation - Sheet3.csv\"\ndataset = load_dataset('csv', data_files=dataset_name, split='train')\n\n# Remove leading/trailing whitespace from column names\ndataset = dataset.rename_columns({col: col.strip() for col in dataset.column_names})\n\n# Verify the column names\nprint(\"Column Names after renaming:\", dataset.column_names)\n\n# Combine 'Question' and 'Answer' into a single text field\nquestion_column = 'প্রশ্ন'\nanswer_column = 'উত্তর'\n\n# Check if the columns exist\nfor col in [question_column, answer_column]:\n    if col not in dataset.column_names:\n        raise ValueError(f\"Column '{col}' not found in the dataset. Available columns are: {dataset.column_names}\")\n\n# Shuffle the dataset\ndataset = dataset.shuffle(seed=42)\n\n# Filter valid examples\ndef is_valid_example(example):\n    question = example[question_column]\n    answer = example[answer_column]\n    return (\n        question is not None and answer is not None and\n        isinstance(question, str) and isinstance(answer, str) and\n        question.strip() != '' and answer.strip() != ''\n    )\n\ndataset = dataset.filter(is_valid_example)\n\n# Combine 'Question' and 'Answer'\ndef combine_questions_answers(example):\n    question = example.get(question_column, '').strip()\n    answer = example.get(answer_column, '').strip()\n    example['text'] = f\"প্রশ্ন: {question}\\nউত্তর: {answer}\"\n    return example\n\ndataset = dataset.map(combine_questions_answers)\n\n# Remove the original columns\ndataset = dataset.remove_columns([question_column, answer_column])\n\n# Verify the combined text\nprint(\"Sample entries after combining 'Question' and 'Answer':\")\nfor i in range(min(3, len(dataset))):\n    print(dataset[i]['text'])\n    print()\n\n# Configure bitsandbytes quantization with CPU offloading\ncompute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=use_nested_quant,\n    bnb_4bit_quant_type=bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype=compute_dtype,\n    load_in_4bit_fp32_cpu_offload=True,  # Enable CPU offloading\n)\n\n# Load the model with empty weights to infer device map\nwith init_empty_weights():\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        quantization_config=bnb_config,\n        trust_remote_code=True,\n    )\n\n# Define maximum memory\nmax_memory = {\n    0: \"14GiB\",  # Adjust based on your GPU's free memory\n    \"cpu\": \"16GiB\",  # Adjust based on your CPU's available RAM\n}\n\n# Infer the device map\nfrom transformers import LlamaConfig\nno_split_modules = [\"LlamaDecoderLayer\"] if isinstance(model.config, LlamaConfig) else []\n\ndevice_map = infer_auto_device_map(\n    model,\n    max_memory=max_memory,\n    no_split_module_classes=no_split_modules,\n)\n\nprint(\"Device Map:\", device_map)\n\n# Reload the model with the device map\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=device_map,\n    trust_remote_code=True,\n)\n\n# Disable caching\nmodel.config.use_cache = False\n\n# Enable gradient checkpointing\nif gradient_checkpointing:\n    model.gradient_checkpointing_enable()\n\n# Prepare LoRA configuration with updated target modules\nmodel = prepare_model_for_kbit_training(model)\n\npeft_config = LoraConfig(\n    r=lora_r,\n    lora_alpha=lora_alpha,\n    target_modules=target_modules,  # Updated target modules\n    lora_dropout=lora_dropout,\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM,\n)\n\n# Apply LoRA to the model\nmodel = get_peft_model(model, peft_config)\n\n# Determine maximum sequence length\nmax_seq_length = min(512, model.config.max_position_embeddings)  # Adjust as needed\nprint(f\"Model can handle maximum {max_seq_length} tokens\")\n\n# Tokenize the dataset\ndef tokenize_function(examples):\n    return tokenizer(\n        examples['text'],\n        truncation=True,\n        max_length=max_seq_length,\n        padding=\"max_length\",\n    )\n\ntokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n\n# Set training parameters\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=per_device_train_batch_size,\n    per_device_eval_batch_size=per_device_eval_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n    fp16=fp16,\n    bf16=bf16,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=group_by_length,\n    lr_scheduler_type=lr_scheduler_type,\n    report_to=\"none\",\n    logging_steps=10,\n    evaluation_strategy=\"no\",\n    save_strategy=\"no\",\n)\n\n# Initialize the SFTTrainer\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=tokenized_dataset,\n    peft_config=peft_config,\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=packing,\n)\n\n# Start training\ntrainer.train()\n\n# Save the fine-tuned model\ntrainer.model.save_pretrained(new_model)\ntokenizer.save_pretrained(new_model)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-28T07:35:02.996698Z","iopub.execute_input":"2024-09-28T07:35:02.997040Z","iopub.status.idle":"2024-09-28T09:53:13.150788Z","shell.execute_reply.started":"2024-09-28T07:35:02.997004Z","shell.execute_reply":"2024-09-28T09:53:13.149813Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ab8658497384418bf1d45d02101ef38"}},"metadata":{}},{"name":"stdout","text":"Column Names after renaming: ['প্রশ্ন', 'উত্তর']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/1879 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"706e679729194a95b83a17a3734d81e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1879 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1120f7015da40a5bda6329e79ef36a6"}},"metadata":{}},{"name":"stderr","text":"Unused kwargs: ['load_in_4bit_fp32_cpu_offload']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n`low_cpu_mem_usage` was None, now set to True since model is quantized.\n","output_type":"stream"},{"name":"stdout","text":"Sample entries after combining 'Question' and 'Answer':\nপ্রশ্ন: বৌ খানের সময় বড্ডা এক্কান আসি দি কইলো, পাক কিরুম অইছে?\nউত্তর: তো তুই কিয়া কইস হিয়ারে?\n\nপ্রশ্ন: এই কথাত্তে চখেত্তে হানি আইয়ের??\nউত্তর: বৌ এর চোখ কান ছলছল\n\nপ্রশ্ন: ওমাহ এইচ্ছা অইলে কেন্নে অইবো??\nউত্তর: হেতে কারো লগে কথা কয়না ,কারো বাসাত যায়না\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0a3522be7d44df8be22a741d99d8475"}},"metadata":{}},{"name":"stdout","text":"Device Map: OrderedDict([('', 0)])\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5d61df0e2a14d2eb08a7a48e2f1c628"}},"metadata":{}},{"name":"stdout","text":"Model can handle maximum 512 tokens\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1879 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26a9c378f45b4c2b80da25c5ea738ebd"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:396: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='469' max='469' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [469/469 2:15:50, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>3.445200</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.997200</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.570500</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.451100</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.357900</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.268800</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>1.296500</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.222900</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>1.222600</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.239400</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>1.177100</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.151600</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>1.118200</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>1.141100</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.129400</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>1.200600</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>1.162200</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>1.116200</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>1.147000</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.055100</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>1.105900</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>1.101600</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>1.029700</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>1.086500</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.101900</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>1.060500</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>1.038500</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>1.040100</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>1.049800</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.014200</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>1.102400</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>1.018400</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>1.020900</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>1.027400</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>1.067600</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>1.077100</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.904200</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>1.039500</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>1.033000</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.023900</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.965200</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.964700</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.941400</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>1.052700</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>1.036700</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.948400</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"('Mistral-7b-QA/tokenizer_config.json',\n 'Mistral-7b-QA/special_tokens_map.json',\n 'Mistral-7b-QA/tokenizer.model',\n 'Mistral-7b-QA/added_tokens.json')"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# Import necessary libraries\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\nimport os\n\n# Define the base model name (same as in your training code)\nmodel_name = \"/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1\"\n\n# Define the path to your fine-tuned model\nfine_tuned_model_path = \"/kaggle/working/Mistral-7b-QA\"\n\n# Create the offload folder if it doesn't exist\noffload_folder = \"/kaggle/working/offload\"\nos.makedirs(offload_folder, exist_ok=True)\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\n    fine_tuned_model_path,\n    trust_remote_code=True,\n    use_fast=False,\n)\n\n# Load the base model with the correct offload parameter\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    trust_remote_code=True,\n    offload_folder=offload_folder,  # Use 'offload_folder' here\n)\n\n# Disable caching to prevent memory issues\nbase_model.config.use_cache = False\n\n# Load the PEFT model with the correct offload parameter\nmodel = PeftModel.from_pretrained(\n    base_model,\n    fine_tuned_model_path,\n    offload_folder=offload_folder,  # Use 'offload_folder' here\n)\n\n# Move the model to the appropriate device (if necessary)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Prepare your prompt\nprompt = \"প্রশ্ন: আইজ্জা অন্যের মনের কিসে?\\nউত্তর:\"\n\n# Tokenize the input prompt\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n\n# Generate the model's response\nwith torch.no_grad():\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=50,       # Adjust as needed\n        do_sample=True,          # Enable sampling for variability\n        temperature=0.7,         # Adjust for creativity vs. coherence\n        top_p=0.9,               # Nucleus sampling\n        repetition_penalty=1.1,  # Penalty to reduce repetition\n        pad_token_id=tokenizer.eos_token_id,  # Avoid warnings about missing pad_token_id\n    )\n\n# Decode the generated tokens\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Print the result\nprint(\"Generated Response:\")\nprint(generated_text)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-28T09:53:13.152130Z","iopub.execute_input":"2024-09-28T09:53:13.152771Z","iopub.status.idle":"2024-09-28T09:53:26.717387Z","shell.execute_reply.started":"2024-09-28T09:53:13.152736Z","shell.execute_reply":"2024-09-28T09:53:26.715886Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4375fe94cd94a9a8dc76c78345b5868"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Move the model to the appropriate device (if necessary)\u001b[39;00m\n\u001b[1;32m     44\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Prepare your prompt\u001b[39;00m\n\u001b[1;32m     48\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mপ্রশ্ন: আইজ্জা অন্যের মনের কিসে?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mউত্তর:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/big_modeling.py:456\u001b[0m, in \u001b[0;36mdispatch_model.<locals>.add_warning.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 456\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt move a model that has some modules offloaded to cpu or disk.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","\u001b[0;31mRuntimeError\u001b[0m: You can't move a model that has some modules offloaded to cpu or disk."],"ename":"RuntimeError","evalue":"You can't move a model that has some modules offloaded to cpu or disk.","output_type":"error"}],"execution_count":5},{"cell_type":"code","source":"# Import necessary libraries\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\nimport os\n\n# Define the base model name (same as in your training code)\nmodel_name = \"/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1\"\n\n# Define the path to your fine-tuned model\nfine_tuned_model_path = \"/kaggle/working/Mistral-7b-QA\"\n\n# Create the offload directory if it doesn't exist\noffload_dir = \"/kaggle/working/offload\"\nos.makedirs(offload_dir, exist_ok=True)\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\n    fine_tuned_model_path,\n    trust_remote_code=True,\n    use_fast=False,\n)\n\n# Load the base model with the correct offload parameter\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    trust_remote_code=True,\n    offload_dir=offload_dir,  # Use 'offload_dir' here\n)\n\n# Disable caching to prevent memory issues\nbase_model.config.use_cache = False\n\n# Load the PEFT model with the correct offload parameter\nmodel = PeftModel.from_pretrained(\n    base_model,\n    fine_tuned_model_path,\n    offload_dir=offload_dir,  # Use 'offload_dir' here\n)\n\n# Remove model.to(device) because the model is already assigned to devices\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# model.to(device)\n\n# Prepare your prompt\nprompt = \"প্রশ্ন: আইজ্জা অন্যের মনের কিসে?\\nউত্তর:\"\n\n# Get the device of the model's first parameter\ndevice = next(iter(model.parameters())).device\n\n# Tokenize the input prompt and move inputs to the correct device\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n\n# Generate the model's response\nwith torch.no_grad():\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=50,       # Adjust as needed\n        do_sample=True,          # Enable sampling for variability\n        temperature=0.7,         # Adjust for creativity vs. coherence\n        top_p=0.9,               # Nucleus sampling\n        repetition_penalty=1.1,  # Penalty to reduce repetition\n        pad_token_id=tokenizer.eos_token_id,  # Avoid warnings about missing pad_token_id\n    )\n\n# Decode the generated tokens\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Print the result\nprint(\"Generated Response:\")\nprint(generated_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-28T10:42:44.240381Z","iopub.execute_input":"2024-09-28T10:42:44.241118Z","iopub.status.idle":"2024-09-28T10:42:44.286178Z","shell.execute_reply.started":"2024-09-28T10:42:44.241076Z","shell.execute_reply":"2024-09-28T10:42:44.284637Z"},"trusted":true},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftModel\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Define the base model name (same as in your training code)\u001b[39;00m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'peft'"],"ename":"ModuleNotFoundError","evalue":"No module named 'peft'","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"# Import necessary libraries\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import PeftModel\nimport os\n\n# Define the base model name\nmodel_name = \"/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1\"\n\n# Define the path to your fine-tuned model\nfine_tuned_model_path = \"/kaggle/working/Mistral-7b-QA\"\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\n    fine_tuned_model_path,\n    trust_remote_code=True,\n    use_fast=False,\n)\n\n# Set up bitsandbytes configuration for 8-bit quantization\nbnb_config = BitsAndBytesConfig(\n    load_in_8bit=True,\n    bnb_8bit_compute_dtype=torch.float16,\n)\n\n# Load the base model with 8-bit quantization\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    torch_dtype=torch.float16,\n    trust_remote_code=True,\n    device_map=\"auto\",\n)\n\n# Disable caching to prevent memory issues\nbase_model.config.use_cache = False\n\n# Load the PEFT model\nmodel = PeftModel.from_pretrained(\n    base_model,\n    fine_tuned_model_path,\n)\n\n# Prepare your prompt\nprompt = \"প্রশ্ন: আইজ্জা অন্যের মনের কিসে?\\nউত্তর:\"\n\n# Get the device of the model's first parameter\ndevice = next(iter(model.parameters())).device\n\n# Tokenize the input prompt and move inputs to the correct device\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n\n# Generate the model's response\nwith torch.no_grad():\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=50,       # Adjust as needed\n        do_sample=True,          # Enable sampling for variability\n        temperature=0.7,         # Adjust for creativity vs. coherence\n        top_p=0.9,               # Nucleus sampling\n        repetition_penalty=1.1,  # Penalty to reduce repetition\n        pad_token_id=tokenizer.eos_token_id,  # Avoid warnings about missing pad_token_id\n    )\n\n# Decode the generated tokens\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Print the result\nprint(\"Generated Response:\")\nprint(generated_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-27T13:35:35.372187Z","iopub.execute_input":"2024-09-27T13:35:35.373136Z","iopub.status.idle":"2024-09-27T13:35:36.234806Z","shell.execute_reply.started":"2024-09-27T13:35:35.373092Z","shell.execute_reply":"2024-09-27T13:35:36.233392Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Unused kwargs: ['bnb_8bit_compute_dtype']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 27\u001b[0m\n\u001b[1;32m     21\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m     22\u001b[0m     load_in_8bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     23\u001b[0m     bnb_8bit_compute_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Load the base model with 8-bit quantization\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m base_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Disable caching to prevent memory issues\u001b[39;00m\n\u001b[1;32m     36\u001b[0m base_model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:3890\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3887\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m infer_auto_device_map(model, dtype\u001b[38;5;241m=\u001b[39mtarget_dtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdevice_map_kwargs)\n\u001b[1;32m   3889\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3890\u001b[0m         \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3892\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3893\u001b[0m     model\u001b[38;5;241m.\u001b[39mtie_weights()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_8bit.py:87\u001b[0m, in \u001b[0;36mBnb8BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m     device_map_without_lm_head \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     84\u001b[0m         key: device_map[key] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules_to_not_convert\n\u001b[1;32m     85\u001b[0m     }\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m---> 87\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     88\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     89\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     90\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     91\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`from_pretrained`. Check \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     92\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor more details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     94\u001b[0m         )\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbitsandbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.37.2\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have a version of `bitsandbytes` that is not compatible with 8bit inference and training\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you have the latest version of `bitsandbytes` installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m     )\n","\u001b[0;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "],"ename":"ValueError","evalue":"Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ","output_type":"error"}],"execution_count":12},{"cell_type":"code","source":"# Import necessary libraries\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\nimport os\n\n# Define the base model name (same as in your training code)\nmodel_name = \"/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1\"\n\n# Define the path to your fine-tuned model\nfine_tuned_model_path = \"/kaggle/working/Mistral-7b-QA\"\n\n# Create the offload folder if it doesn't exist\noffload_folder = \"/kaggle/working/offload\"\nos.makedirs(offload_folder, exist_ok=True)\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\n    fine_tuned_model_path,\n    trust_remote_code=True,\n    use_fast=False,\n)\n\n# Load the base model with the correct offload parameter\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    trust_remote_code=True,\n    offload_folder=offload_folder,  # Use 'offload_folder' here\n)\n\n# Disable caching to prevent memory issues\nbase_model.config.use_cache = False\n\n# Load the PEFT model with the correct offload parameter\nmodel = PeftModel.from_pretrained(\n    base_model,\n    fine_tuned_model_path,\n    offload_folder=offload_folder,  # Use 'offload_folder' here\n)\n\n# Move the model to the appropriate device (if necessary)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Prepare your prompt\nprompt = \"প্রশ্ন: আইজ্জা অন্যের মনের কিসে?\\nউত্তর:\"\n\n# Tokenize the input prompt\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n\n# Generate the model's response\nwith torch.no_grad():\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=50,       # Adjust as needed\n        do_sample=True,          # Enable sampling for variability\n        temperature=0.7,         # Adjust for creativity vs. coherence\n        top_p=0.9,               # Nucleus sampling\n        repetition_penalty=1.1,  # Penalty to reduce repetition\n        pad_token_id=tokenizer.eos_token_id,  # Avoid warnings about missing pad_token_id\n    )\n\n# Decode the generated tokens\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Print the result\nprint(\"Generated Response:\")\nprint(generated_text)","metadata":{"execution":{"iopub.status.busy":"2024-09-28T10:43:48.419629Z","iopub.execute_input":"2024-09-28T10:43:48.420258Z","iopub.status.idle":"2024-09-28T10:43:49.350884Z","shell.execute_reply.started":"2024-09-28T10:43:48.420223Z","shell.execute_reply":"2024-09-28T10:43:49.349500Z"},"trusted":true},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:402\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:101\u001b[0m, in \u001b[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:154\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m     )\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n","\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/kaggle/working/Mistral-7b-QA'. Use `repo_type` argument if needed.","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(offload_folder, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Load the tokenizer\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfine_tuned_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_fast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Load the base model with the correct offload parameter\u001b[39;00m\n\u001b[1;32m     25\u001b[0m base_model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     26\u001b[0m     model_name,\n\u001b[1;32m     27\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m     offload_folder\u001b[38;5;241m=\u001b[39moffload_folder,  \u001b[38;5;66;03m# Use 'offload_folder' here\u001b[39;00m\n\u001b[1;32m     31\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:834\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    831\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    833\u001b[0m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[0;32m--> 834\u001b[0m tokenizer_config \u001b[38;5;241m=\u001b[39m \u001b[43mget_tokenizer_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[1;32m    836\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:666\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    663\u001b[0m     token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[1;32m    665\u001b[0m commit_hash \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 666\u001b[0m resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    683\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:466\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere was a specific connection error when trying to load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HFValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    467\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect path_or_model_id: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please provide either the path to a local folder or the repo_id of a model on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    468\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n","\u001b[0;31mOSError\u001b[0m: Incorrect path_or_model_id: '/kaggle/working/Mistral-7b-QA'. Please provide either the path to a local folder or the repo_id of a model on the Hub."],"ename":"OSError","evalue":"Incorrect path_or_model_id: '/kaggle/working/Mistral-7b-QA'. Please provide either the path to a local folder or the repo_id of a model on the Hub.","output_type":"error"}],"execution_count":6},{"cell_type":"code","source":"# Prepare your prompt\nprompt = \"প্রশ্ন: আইজ্জা অন্যের মনের কিসে?\\nউত্তর:\"\n\n# Tokenize the input prompt\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n\n# Generate the model's response\nwith torch.no_grad():\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=50,       # Adjust as needed\n        do_sample=True,          # Enable sampling for variability\n        temperature=0.7,         # Adjust for creativity vs. coherence\n        top_p=0.9,               # Nucleus sampling\n        repetition_penalty=1.1,  # Penalty to reduce repetition\n        pad_token_id=tokenizer.eos_token_id,  # Avoid warnings about missing pad_token_id\n    )\n\n# Decode the generated tokens\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Print the result\nprint(\"Generated Response:\")\nprint(generated_text)","metadata":{"execution":{"iopub.status.busy":"2024-09-28T10:40:18.497504Z","iopub.status.idle":"2024-09-28T10:40:18.497891Z","shell.execute_reply.started":"2024-09-28T10:40:18.497717Z","shell.execute_reply":"2024-09-28T10:40:18.497736Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prepare your prompt\nprompt = \"প্রশ্ন: থাকগোই বাদ দে এলগা?\\nউত্তর:\"\n# Print the result\nprint(\"Generated Response:\")\nprint(generated_text)","metadata":{"execution":{"iopub.status.busy":"2024-09-28T10:40:18.499465Z","iopub.status.idle":"2024-09-28T10:40:18.499856Z","shell.execute_reply.started":"2024-09-28T10:40:18.499677Z","shell.execute_reply":"2024-09-28T10:40:18.499696Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install the evaluate library\n!pip install evaluate --quiet\n\n# Restart the kernel after installation (uncomment the following lines if running interactively)\n# import sys\n# sys.exit()\n\n# Import necessary libraries for evaluation\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\nimport numpy as np\nfrom datasets import load_dataset\nimport evaluate\n\n# Define the path to your fine-tuned model\nnew_model = \"Mistral-7b-QA\"  # Replace with your actual path if different\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\n    new_model,\n    trust_remote_code=True,\n    use_fast=False,\n)\n\n# Load the base model (same as used during training)\nmodel_name = \"/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1\"\n\n# Load the base model\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    trust_remote_code=True,\n)\n\n# Disable caching to prevent memory issues\nbase_model.config.use_cache = False\n\n# Load the fine-tuned model with LoRA adapters\nmodel = PeftModel.from_pretrained(\n    base_model,\n    new_model,\n)\n\n# Move the model to the appropriate device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Prepare the test dataset\n# Let's assume you have the 'dataset' variable from your code\n# If not, reload your dataset\ndataset_name = \"/kaggle/input/noahkalifinal/Noakhali Train Translation - Sheet3.csv\"\ndataset = load_dataset('csv', data_files=dataset_name, split='train')\n\n# Remove leading/trailing whitespace from column names\ndataset = dataset.rename_columns({col: col.strip() for col in dataset.column_names})\n\n# Combine 'Question' and 'Answer' into a single text field\nquestion_column = 'Question'\nanswer_column = 'Answer'\n\n# Filter valid examples (reuse your `is_valid_example` function)\ndef is_valid_example(example):\n    question = example[question_column]\n    answer = example[answer_column]\n    return (\n        question is not None and answer is not None and\n        isinstance(question, str) and isinstance(answer, str) and\n        question.strip() != '' and answer.strip() != ''\n    )\n\ndataset = dataset.filter(is_valid_example)\n\n# Combine 'Question' and 'Answer'\ndef combine_questions_answers(example):\n    question = example.get(question_column, '').strip()\n    answer = example.get(answer_column, '').strip()\n    example['text'] = f\"প্রশ্ন: {question}\\nউত্তর: {answer}\"\n    return example\n\ndataset = dataset.map(combine_questions_answers)\ndataset = dataset.remove_columns([question_column, answer_column])\n\n# Split the dataset into train and test sets\nsplit_dataset = dataset.train_test_split(test_size=0.2, seed=42)\ntrain_dataset = split_dataset['train']\ntest_dataset = split_dataset['test']\n\n# Prepare the test dataset for evaluation\ndef prepare_test_examples(examples):\n    questions = []\n    answers = []\n    for text in examples['text']:\n        if 'প্রশ্ন:' in text and '\\nউত্তর:' in text:\n            q_a = text.split('\\nউত্তর:')\n            question = q_a[0]\n            answer = q_a[1]\n            questions.append(question + '\\nউত্তর:')\n            answers.append(answer.strip())\n        else:\n            questions.append(text)\n            answers.append('')\n    return {'questions': questions, 'answers': answers}\n\ntest_dataset = test_dataset.map(prepare_test_examples, batched=True, remove_columns=['text'])\n\n# Generate predictions\nfrom tqdm import tqdm\n\npredictions = []\nreferences = []\n\nfor i in tqdm(range(len(test_dataset))):\n    question = test_dataset[i]['questions']\n    reference = test_dataset[i]['answers']\n    \n    # Tokenize the question\n    inputs = tokenizer(question, return_tensors='pt').to(device)\n    \n    # Generate the model's response\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=50,\n            do_sample=True,\n            temperature=0.7,\n            top_p=0.9,\n            repetition_penalty=1.1,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    # Decode the generated response\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Extract the answer part\n    if 'উত্তর:' in generated_text:\n        generated_answer = generated_text.split('উত্তর:')[1].strip()\n    else:\n        generated_answer = generated_text.strip()\n    \n    predictions.append(generated_answer)\n    references.append(reference)\n\n# Compute evaluation metrics\n# We'll compute Exact Match (EM), BLEU, and ROUGE scores\n\n# Exact Match\nexact_matches = [int(pred.strip() == ref.strip()) for pred, ref in zip(predictions, references)]\nem_score = np.mean(exact_matches) * 100\nprint(f\"Exact Match (EM) Score: {em_score:.2f}%\")\n\n# BLEU Score\nbleu_metric = evaluate.load('bleu')\nbleu_score = bleu_metric.compute(predictions=predictions, references=references)['bleu'] * 100\nprint(f\"BLEU Score: {bleu_score:.2f}%\")\n\n# ROUGE Score\nrouge_metric = evaluate.load('rouge')\nrouge_scores = rouge_metric.compute(predictions=predictions, references=references)\nprint(\"ROUGE Scores:\")\nfor key in rouge_scores:\n    score = rouge_scores[key].mid.fmeasure * 100\n    print(f\"{key}: {score:.2f}%\")\n\n# You can also print some sample predictions\nfor i in range(5):\n    print(f\"Question: {test_dataset[i]['questions']}\")\n    print(f\"True Answer: {test_dataset[i]['answers']}\")\n    print(f\"Generated Answer: {predictions[i]}\")\n    print('-' * 50)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-28T10:39:58.105709Z","iopub.execute_input":"2024-09-28T10:39:58.106415Z","iopub.status.idle":"2024-09-28T10:40:18.496362Z","shell.execute_reply.started":"2024-09-28T10:39:58.106371Z","shell.execute_reply":"2024-09-28T10:40:18.494676Z"},"trusted":true},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftModel\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'peft'"],"ename":"ModuleNotFoundError","evalue":"No module named 'peft'","output_type":"error"}],"execution_count":1}]}